# Fluentd配置文件 - 财务系统日志聚合

# 系统配置
<system>
  workers 2
  root_dir /var/log/fluentd
  log_level info
  suppress_repeated_stacktrace true
</system>

# 输入源配置
<source>
  @type tail
  @id financial_backend_logs
  tag financial.backend.*
  path /var/log/containers/financial-backend-*.log
  pos_file /var/log/fluentd/financial-backend.log.pos
  <parse>
    @type json
    time_key timestamp
    time_format %Y-%m-%dT%H:%M:%S.%LZ
    keep_time_key true
  </parse>
  refresh_interval 5
  read_from_head true
  follow_inodes true
</source>

<source>
  @type tail
  @id financial_frontend_logs
  tag financial.frontend.*
  path /var/log/containers/financial-frontend-*.log
  pos_file /var/log/fluentd/financial-frontend.log.pos
  <parse>
    @type json
    time_key timestamp
    time_format %Y-%m-%dT%H:%M:%S.%LZ
    keep_time_key true
  </parse>
  refresh_interval 5
  read_from_head true
</source>

# PostgreSQL日志
<source>
  @type tail
  @id postgresql_logs
  tag database.postgresql.*
  path /var/log/postgresql/postgresql-*.log
  pos_file /var/log/fluentd/postgresql.log.pos
  <parse>
    @type regexp
    expression /^(?<time>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}.\d{3} \w+) \[(?<pid>\d+)\] (?<level>\w+): (?<message>.*)/
    time_key time
    time_format %Y-%m-%d %H:%M:%S.%L %Z
  </parse>
</source>

# Redis日志
<source>
  @type tail
  @id redis_logs
  tag cache.redis.*
  path /var/log/redis/redis-server.log
  pos_file /var/log/fluentd/redis.log.pos
  <parse>
    @type regexp
    expression /^(?<pid>\d+):(?<role>\w) (?<time>\d{2} \w{3} \d{4} \d{2}:\d{2}:\d{2}.\d{3}) (?<level>[*#.-]) (?<message>.*)/
    time_key time
    time_format %d %b %Y %H:%M:%S.%L
  </parse>
</source>

# Nginx访问日志
<source>
  @type tail
  @id nginx_access_logs
  tag nginx.access.*
  path /var/log/nginx/access.log
  pos_file /var/log/fluentd/nginx-access.log.pos
  <parse>
    @type nginx
    keep_time_key true
  </parse>
</source>

# Nginx错误日志
<source>
  @type tail
  @id nginx_error_logs
  tag nginx.error.*
  path /var/log/nginx/error.log
  pos_file /var/log/fluentd/nginx-error.log.pos
  <parse>
    @type regexp
    expression /^(?<time>\d{4}/\d{2}/\d{2} \d{2}:\d{2}:\d{2}) \[(?<level>\w+)\] (?<pid>\d+)#(?<tid>\d+): (?<message>.*)/
    time_key time
    time_format %Y/%m/%d %H:%M:%S
  </parse>
</source>

# 系统日志
<source>
  @type systemd
  @id systemd_logs
  tag system.*
  path /var/log/journal
  <storage>
    @type local
    persistent true
    path /var/log/fluentd/systemd.pos
  </storage>
  <entry>
    fields_strip_underscores true
    field_map {"MESSAGE": "message", "_SYSTEMD_UNIT": "unit", "_PID": "pid", "PRIORITY": "level"}
  </entry>
</source>

# Kubernetes事件
<source>
  @type kubernetes_events
  @id kubernetes_events
  tag kubernetes.events.*
</source>

# 过滤器配置
<filter financial.**>
  @type record_transformer
  <record>
    hostname "#{Socket.gethostname}"
    service_name ${tag_parts[1]}
    environment "#{ENV['ENVIRONMENT'] || 'unknown'}"
    cluster_name "#{ENV['CLUSTER_NAME'] || 'financial-system'}"
  </record>
</filter>

# 添加Kubernetes元数据
<filter financial.**>
  @type kubernetes_metadata
  @id kubernetes_metadata
  kubernetes_url "#{ENV['KUBERNETES_SERVICE_HOST']}:#{ENV['KUBERNETES_SERVICE_PORT_HTTPS']}"
  verify_ssl true
  ca_file /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
  bearer_token_file /var/run/secrets/kubernetes.io/serviceaccount/token
  cache_size 1000
  cache_ttl 3600
  watch true
</filter>

# 错误日志特殊处理
<filter financial.backend.**>
  @type grep
  <regexp>
    key level
    pattern ^(ERROR|FATAL|CRITICAL)$
  </regexp>
  <record>
    alert_required true
    severity high
  </record>
</filter>

# 性能日志处理
<filter financial.**>
  @type parser
  key_name message
  reserve_data true
  <parse>
    @type regexp
    expression /response_time=(?<response_time>\d+\.?\d*)ms/
  </parse>
</filter>

# 安全日志过滤
<filter financial.**>
  @type grep
  <regexp>
    key message
    pattern /(login|authentication|authorization|security|breach|attack)/i
  </regexp>
  <record>
    log_type security
    requires_review true
  </record>
</filter>

# 业务指标提取
<filter financial.backend.**>
  @type parser
  key_name message
  reserve_data true
  <parse>
    @type regexp
    expression /transaction_id=(?<transaction_id>[a-zA-Z0-9-]+)\s+amount=(?<transaction_amount>\d+\.?\d*)\s+user_id=(?<user_id>\d+)/
  </parse>
</filter>

# 输出配置

# 错误日志输出到Elasticsearch
<match financial.** database.** cache.**>
  @type elasticsearch
  @id elasticsearch_financial
  host "#{ENV['ELASTICSEARCH_HOST'] || 'elasticsearch'}"
  port "#{ENV['ELASTICSEARCH_PORT'] || '9200'}"
  scheme "#{ENV['ELASTICSEARCH_SCHEME'] || 'http'}"
  user "#{ENV['ELASTICSEARCH_USER']}"
  password "#{ENV['ELASTICSEARCH_PASSWORD']}"
  
  index_name financial-logs-%Y.%m.%d
  type_name _doc
  
  # 模板配置
  template_name financial-logs
  template_file /fluentd/etc/elasticsearch-template.json
  template_overwrite true
  
  # 缓冲配置
  <buffer time>
    @type file
    path /var/log/fluentd/elasticsearch
    timekey 1h
    timekey_wait 5m
    timekey_use_utc true
    chunk_limit_size 64MB
    total_limit_size 1GB
    flush_mode interval
    flush_interval 10s
    flush_thread_count 4
    retry_max_interval 30
    retry_forever false
    retry_max_times 3
  </buffer>
  
  # 错误处理
  <secondary>
    @type file
    path /var/log/fluentd/failed_records
    append true
    <buffer>
      flush_mode immediate
    </buffer>
  </secondary>
</match>

# 系统日志输出
<match system.**>
  @type elasticsearch
  @id elasticsearch_system
  host "#{ENV['ELASTICSEARCH_HOST'] || 'elasticsearch'}"
  port "#{ENV['ELASTICSEARCH_PORT'] || '9200'}"
  scheme "#{ENV['ELASTICSEARCH_SCHEME'] || 'http'}"
  
  index_name system-logs-%Y.%m.%d
  type_name _doc
  
  <buffer time>
    timekey 1h
    timekey_wait 5m
    chunk_limit_size 32MB
    flush_interval 30s
  </buffer>
</match>

# Nginx日志输出
<match nginx.**>
  @type elasticsearch
  @id elasticsearch_nginx
  host "#{ENV['ELASTICSEARCH_HOST'] || 'elasticsearch'}"
  port "#{ENV['ELASTICSEARCH_PORT'] || '9200'}"
  
  index_name nginx-logs-%Y.%m.%d
  type_name _doc
  
  <buffer time>
    timekey 1h
    timekey_wait 5m
    chunk_limit_size 32MB
    flush_interval 15s
  </buffer>
</match>

# Kubernetes事件输出
<match kubernetes.events.**>
  @type elasticsearch
  @id elasticsearch_k8s_events
  host "#{ENV['ELASTICSEARCH_HOST'] || 'elasticsearch'}"
  port "#{ENV['ELASTICSEARCH_PORT'] || '9200'}"
  
  index_name k8s-events-%Y.%m.%d
  type_name _doc
  
  <buffer time>
    timekey 1h
    timekey_wait 5m
    chunk_limit_size 16MB
    flush_interval 60s
  </buffer>
</match>

# 告警日志输出到专门的索引
<match **>
  @type copy
  <store>
    @type elasticsearch
    @id elasticsearch_alerts
    host "#{ENV['ELASTICSEARCH_HOST'] || 'elasticsearch'}"
    port "#{ENV['ELASTICSEARCH_PORT'] || '9200'}"
    
    index_name alerts-%Y.%m.%d
    type_name _doc
    
    # 只记录需要告警的日志
    <filter>
      @type grep
      <regexp>
        key alert_required
        pattern true
      </regexp>
    </filter>
    
    <buffer time>
      timekey 1h
      timekey_wait 5m
      chunk_limit_size 16MB
      flush_interval 30s
    </buffer>
  </store>
  
  # 备份到文件
  <store>
    @type file
    @id file_backup
    path /var/log/fluentd/backup/%Y/%m/%d/financial-logs
    append true
    time_slice_format %Y%m%d%H
    
    <buffer time>
      timekey 1h
      timekey_wait 5m
      chunk_limit_size 256MB
      flush_interval 300s
    </buffer>
    
    <format>
      @type json
    </format>
  </store>
</match>

# 指标输出到Prometheus
<match metrics.**>
  @type prometheus
  @id prometheus_metrics
  
  <metric>
    name fluentd_input_status_num_records_total
    type counter
    desc The total number of incoming records
    <labels>
      tag ${tag}
      hostname ${hostname}
    </labels>
  </metric>
  
  <metric>
    name fluentd_output_status_num_records_total
    type counter
    desc The total number of outgoing records
    <labels>
      tag ${tag}
      hostname ${hostname}
    </labels>
  </metric>
</match>

# 健康检查
<source>
  @type http
  @id fluentd_http_health
  port 9880
  bind 0.0.0.0
</source>

# 监控指标
<source>
  @type monitor_agent
  @id fluentd_monitor
  bind 0.0.0.0
  port 24220
  tag fluentd.monitor
</source>